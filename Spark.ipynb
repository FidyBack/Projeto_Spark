{
  "metadata": {
    "name": "Tarefa_1",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrdd \u003d sc.textFile(\u0027s3://megadados-alunos/dados/all_reviews_clean_tsv/\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrdd \u003d rdd.cache()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntotal_reviews \u003d rdd.count()\nprint(f\"Reviews: {total_reviews}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nall_clients \u003d rdd \\\n    .map(lambda x: x.split(\"\\t\")) \\\n    .map(lambda x: x[1]) \\\n    .cache() \\\n    .distinct() \\\n    .count()\n\nprint(f\"Clientes: {all_clients}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nall_products \u003d rdd \\\n    .map(lambda x: x.split(\"\\t\")) \\\n    .map(lambda x: x[3]) \\\n    .distinct() \\\n    .cache() \\\n    .count()\n    \nprint(f\"Produtos: {all_products}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nbasic_op \u003d rdd \\\n    .map(lambda x: x.split(\"\\t\")) \\\n    .map(lambda x: x[7]) \\\n    .cache()\n    \none_star \u003d basic_op \\\n    .filter(lambda x: x \u003d\u003d \"1\") \\\n    .count()\n    \ntwo_star \u003d basic_op \\\n    .filter(lambda x: x \u003d\u003d \"2\") \\\n    .count()\n    \nthree_star \u003d basic_op \\\n    .filter(lambda x: x \u003d\u003d \"3\") \\\n    .count()\n    \nfour_star \u003d basic_op \\\n    .filter(lambda x: x \u003d\u003d \"4\") \\\n    .count()\n    \nfive_star \u003d basic_op \\\n    .filter(lambda x: x \u003d\u003d \"5\") \\\n    .count()\n    \nprint(f\"Uma Estrela: {one_star}\\nDuas Estrelas: {two_star}\\nTrês Estrelas: {three_star}\\nQuatro Estrelas: {four_star}\\nCinco Estrelas: {five_star}\\n\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nmore_than_one_review \u003d rdd \\\n    .map(lambda x: x.split(\"\\t\")) \\\n    .map(lambda x: (x[1], 1)) \\\n    .reduceByKey(lambda x, y: x + y) \\\n    .filter(lambda x: x[1] !\u003d 1)\n    \nall_reviews \u003d rdd \\\n    .map(lambda x: x.split(\"\\t\")) \\\n    .map(lambda x: (x[1], x))\n    \nsubtract_all \u003d all_reviews \\\n    .subtractByKey(more_than_one_review) \\\n    .cache() \\\n    .map(lambda x: x[1])\n    \n\nless_than_ten_reviews \u003d subtract_all \\\n    .map(lambda x: (x[3], 1)) \\\n    .reduceByKey(lambda x, y: x + y) \\\n    .filter(lambda x: x[1] \u003c 10)\n    \nall_new_reviews \u003d subtract_all \\\n    .map(lambda x: (x[3], x))\n    \nsubtract_products \u003d all_new_reviews \\\n    .subtractByKey(less_than_ten_reviews) \\\n    .cache() \\\n    .map(lambda x: x[1])\n    \n\nproduct_mean \u003d subtract_products \\\n    .map(lambda x: (x[3], x[7])) \\\n    .groupByKey() \\\n    .mapValues(list) \\\n    .map(lambda x: (x[0], sum([int(i) for i in x[1]])/len(x[1])))\n\nproducts \u003d rdd \\\n    .map(lambda x: x.split(\"\\t\")) \\\n    .map(lambda x: (x[3], x[5])) \\\n    .distinct()\n\nname \u003d product_mean \\\n    .join(products) \\\n    .sortBy(lambda x: x[1][0], ascending\u003dFalse)\n\nprint(f\"10 produtos: {name.take(10)}\")\nprint(f\"Produtos 5 estrelas: {name.filter(lambda x: x[1][0] \u003d\u003d 5).count()}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrdd_2 \u003d rdd \\\n    .map(lambda x: x.split(\"\\t\")) \\\n    .map(lambda x: (x[1], 1)) \\\n    .reduceByKey(lambda x, y: x + y)\n    \n  \navr_review_per_person \u003d rdd_2 \\\n    .map(lambda x: x[1]) \\\n    .mean()\n    \nrobot_suspect \u003d rdd_2 \\\n    .filter(lambda x: 100*avr_review_per_person \u003c x[1] \u003c 1000*avr_review_per_person)\n    \npretty_sure_robot \u003d rdd_2 \\\n    .filter(lambda x: x[1] \u003e 1000*avr_review_per_person)\n\nprint(f\"Média de Reviews por Usuários: {avr_review_per_person}\\nRobos Suspeitos: {robot_suspect.count()}\\nRobos Muito Suspeitos: {pretty_sure_robot.count()}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfiltred_reviews_bot \u003d rdd \\\n    .map(lambda x: x.split(\"\\t\")) \\\n    .map(lambda x: (x[1], x[7])) \\\n    .cache() \\\n    .join(pretty_sure_robot)\n\nreview_bot \u003d filtred_reviews_bot \\\n    .map(lambda x: ((x[0], x[1][1]), x[1][0])) \\\n    .groupByKey() \\\n    .mapValues(list) \\\n    .map(lambda x: (x[0], sum([int(i) for i in x[1]])/len(x[1]))) \\\n    .sortBy(lambda x: x[1], ascending\u003dFalse)\n    \nmax_bots \u003d review_bot \\\n    .map(lambda x: x[1]) \\\n    .max()\n    \nmin_bots \u003d review_bot \\\n    .map(lambda x: x[1]) \\\n    .min()\n    \nbot_avg \u003d review_bot \\\n    .map(lambda x: x[1]) \\\n    .mean()\n\n\nfiltred_reviews_middle_bots \u003d rdd \\\n    .map(lambda x: x.split(\"\\t\")) \\\n    .map(lambda x: (x[1], x[7])) \\\n    .join(robot_suspect)\n\nreview_middle_bot \u003d filtred_reviews_middle_bots \\\n    .map(lambda x: ((x[0], x[1][1]), x[1][0])) \\\n    .groupByKey() \\\n    .mapValues(list) \\\n    .map(lambda x: (x[0], sum([int(i) for i in x[1]])/len(x[1]))) \\\n    .sortBy(lambda x: x[1], ascending\u003dFalse)\n    \nmax_middle_bots \u003d review_middle_bot \\\n    .map(lambda x: x[1]) \\\n    .max()\n    \nmin_middle_bots \u003d review_middle_bot \\\n    .map(lambda x: x[1]) \\\n    .min()\n    \nmiddle_bot_avg \u003d review_middle_bot \\\n    .map(lambda x: x[1]) \\\n    .mean()\n    "
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(f\"Robots\\nMédia de review {bot_avg}\\nReview Máximo {max_bots}\\nReview Mínimo {min_bots}\\n\")\nprint(f\"Suspeitos de Bots\\nMédia de review {middle_bot_avg}\\nReview Máximo {max_middle_bots}\\nReview Mínimo {min_middle_bots}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfiltred_class_bot \u003d rdd \\\n    .map(lambda x: x.split(\"\\t\")) \\\n    .map(lambda x: (x[1], x[6])) \\\n    .cache() \\\n    .join(pretty_sure_robot)\n    \nclass_bot \u003d filtred_class_bot \\\n    .map(lambda x: ((x[0], x[1][1]), x[1][0])) \\\n    .groupByKey() \\\n    .mapValues(list) \\\n    .map(lambda x: (x[0], list(set([(i, x[1].count(i)) for i in x[1]])))) \\\n    .map(lambda x: x[1]) \\\n    .flatMap(lambda x: x) \\\n    .groupByKey() \\\n    .mapValues(sum) \\\n    .sortBy(lambda x: x[1], ascending\u003dFalse)\n    \n\nfiltred_middle_class_bot \u003d rdd \\\n    .map(lambda x: x.split(\"\\t\")) \\\n    .map(lambda x: (x[1], x[6])) \\\n    .join(robot_suspect)\n    \nmiddle_class_bot \u003d filtred_middle_class_bot \\\n    .map(lambda x: ((x[0], x[1][1]), x[1][0])) \\\n    .groupByKey() \\\n    .mapValues(list) \\\n    .map(lambda x: (x[0], list(set([(i, x[1].count(i)) for i in x[1]])))) \\\n    .map(lambda x: x[1]) \\\n    .flatMap(lambda x: x) \\\n    .groupByKey() \\\n    .mapValues(sum) \\\n    .sortBy(lambda x: x[1], ascending\u003dFalse)\n    \n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(f\"Robots\\nCategorias mais Classificadas: {class_bot.take(10)}\\n\")\nprint(f\"Suspeitos de Bots\\nCategorias mais Classificadas: {middle_class_bot.take(10)}\\n\")\n    "
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import when\n\ndf \u003d spark.read.option(\"header\", \"false\") \\\n    .option(\"delimiter\", \"\\t\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .csv(\"s3://megadados-alunos/dados/all_reviews_clean_tsv/\") \\\n    .withColumnRenamed(\"_c0\", \"marketplace\") \\\n    .withColumnRenamed(\"_c1\", \"customer_id\") \\\n    .withColumnRenamed(\"_c2\", \"review_id\") \\\n    .withColumnRenamed(\"_c3\", \"product_id\") \\\n    .withColumnRenamed(\"_c4\", \"product_parent\") \\\n    .withColumnRenamed(\"_c5\", \"product_title\") \\\n    .withColumnRenamed(\"_c6\", \"product_category\") \\\n    .withColumnRenamed(\"_c7\", \"star_rating\") \\\n    .withColumnRenamed(\"_c8\", \"helpful_votes\") \\\n    .withColumnRenamed(\"_c9\", \"total_votes\") \\\n    .withColumnRenamed(\"_c10\", \"vine\") \\\n    .withColumnRenamed(\"_c11\", \"verified_purchase\") \\\n    .withColumnRenamed(\"_c12\", \"review_headline\") \\\n    .withColumnRenamed(\"_c13\", \"review_body\") \\\n    .withColumnRenamed(\"_c14\", \"review_date\") \\\n    .dropna()\n    \ndf \u003d df.withColumn(\"label_rating\", when(df.star_rating \u003d\u003d 1, \"Negativa\").when(df.star_rating \u003d\u003d 2, \"Negativa\").when(df.star_rating \u003d\u003d 3, \"Negativa\").when(df.star_rating \u003d\u003d 4, \"Neutra\").when(df.star_rating \u003d\u003d 5, \"Positiva\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.feature import CountVectorizer\nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import NaiveBayes"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nstages \u003d []\n# 1. clean data and tokenize sentences using RegexTokenizer\nregexTokenizer \u003d RegexTokenizer(inputCol\u003d\"review_body\", outputCol\u003d\"tokens\", pattern\u003d\"\\\\W+\")\nstages +\u003d [regexTokenizer]\n\n# 2. CountVectorize the data\ncv \u003d CountVectorizer(inputCol\u003d\"tokens\", outputCol\u003d\"token_features\", minDF\u003d2.0)#, vocabSize\u003d3, minDF\u003d2.0\nstages +\u003d [cv]\n\n# 3. Convert the labels to numerical values using binariser\nindexer \u003d StringIndexer(inputCol\u003d\"label_rating\", outputCol\u003d\"label\")\nstages +\u003d [indexer]\n\n# 4. Vectorise features using vectorassembler\nvecAssembler \u003d VectorAssembler(inputCols\u003d[\u0027token_features\u0027], outputCol\u003d\"features\")\nstages +\u003d [vecAssembler]\n\n[print(\u0027\\n\u0027, stage) for stage in stages]"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml import Pipeline\npipeline \u003d Pipeline(stages\u003dstages)\ndata \u003d pipeline.fit(df).transform(df)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntrain, test \u003d data.randomSplit([0.7, 0.3], seed \u003d 2018)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.classification import NaiveBayes\n# Initialise the model\nnb \u003d NaiveBayes(smoothing\u003d1.0, modelType\u003d\"multinomial\")\n# Fit the model\nmodel \u003d nb.fit(train)\n# Make predictions on test data\npredictions \u003d model.transform(test)\npredictions.select(\"label\", \"prediction\", \"probability\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator \u003d BinaryClassificationEvaluator(rawPredictionCol\u003d\"prediction\")\naccuracy \u003d evaluator.evaluate(predictions)\nprint (\"Model Accuracy: \", accuracy)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}